{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPZacBV07Bo0aHI5TgLMciA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eq2OylGszckB","executionInfo":{"status":"ok","timestamp":1694845858363,"user_tz":300,"elapsed":13044,"user":{"displayName":"Everth Palomino","userId":"05447232039002194558"}},"outputId":"13b3db03-3325-4c31-d83c-8e5b7a59aa6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["#Instalacion spark\n","!pip install pyspark"]},{"cell_type":"code","source":["from google.colab import drive\n","from pyspark.sql import SparkSession\n","from pyspark.context import SparkContext\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from pyspark.sql import Row\n","from datetime import *\n","import pandas as pd\n","import os\n","import re\n","import shutil"],"metadata":{"id":"-nDZ9Yxm3fdw","executionInfo":{"status":"ok","timestamp":1694849033912,"user_tz":300,"elapsed":2,"user":{"displayName":"Everth Palomino","userId":"05447232039002194558"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["#Montando la ruta de google drive para el almacenamiento de los archivos\n","drive.mount('/content/drive')\n","spark = SparkSession.builder.master(\"local[1]\").appName(\"ChallengeN5\").getOrCreate()\n","sc=spark.sparkContext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkBbu6IcEbry","executionInfo":{"status":"ok","timestamp":1694845931319,"user_tz":300,"elapsed":14794,"user":{"displayName":"Everth Palomino","userId":"05447232039002194558"}},"outputId":"97d75e94-94f9-49af-e6ab-d92b52d2e282"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#Clase que maneja todas las operaciones para el cargado de los archivos.\n","class Challenge:\n","  def __init__(self,ruta_origen,ruta_raw,ruta_curated,fec_proceso,tipo_carga):\n","    self.ruta_origen=ruta_origen\n","    self.ruta_raw=ruta_raw\n","    self.ruta_curated=ruta_curated\n","    self.fec_proceso = fec_proceso\n","    self.tipo_carga = tipo_carga\n","  def valida_archivos_carga(self):\n","    with os.scandir(self.ruta_origen) as ficheros:\n","      lst_ficheros=[fichero.name for fichero in ficheros if fichero.is_file()]\n","    return lst_ficheros\n","  #Carga de archivos en carpeta raw en formato parquet, despues de pasarlo a parquet lo mueve a una ruta de procesado.\n","  def cargaraw(self,nombrearchivo):\n","    df_archivo=pd.read_csv(self.ruta_origen+nombrearchivo)\n","    df_archivo.to_parquet(self.ruta_raw+nombrearchivo.replace('csv','parquet'))\n","    shutil.move(self.ruta_origen+nombrearchivo,f'{self.ruta_origen}procesado/{nombrearchivo.replace(\".csv\",str(self.fec_proceso))}.csv')\n","\n","  #Lee los archivos guardados en formato parquet.\n","  def readfile(self,nomfile):\n","    df_archivo=spark.read.parquet(self.ruta_raw+nomfile.replace('csv','parquet'))\n","    return df_archivo\n","\n","  #Procesa la informacion cargada y genera un archivo resumen.\n","  def proceso_data(self,df_full_grouped , df_covid_clean):\n","    dff_full_grouped= df_full_grouped.withColumnsRenamed({'Date':'date_group','Country/Region':'country_group','WHO Region':'region','New cases':'new_cases','New deaths':'new_deaths','New recovered':'new_recovered'}) \\\n","    .select('date_group','region','country_group','Confirmed','Deaths','Recovered','Active','new_cases','new_deaths','new_recovered')\n","\n","    dff_covid_clean=df_covid_clean.withColumnsRenamed({'Province/State':'province','Lat':'latitude','Long':'Longitude','Date':'date_covid','Country/Region':'country_covid'}) \\\n","    .select('date_covid','province','latitude','longitude','country_covid')\n","\n","    df_resumen = dff_covid_clean.join(dff_full_grouped,(dff_covid_clean.date_covid==dff_full_grouped.date_group) & (dff_covid_clean.country_covid ==dff_full_grouped.country_group)) \\\n","    .select(col('date_group').alias('date'),'region',col('country_group').alias('country'),'province','latitude','longitude','Confirmed','Deaths','Recovered','Active' \\\n","            ,'new_cases','new_deaths','new_recovered')\n","\n","    dff_resumen = df_resumen.withColumn('Confirmed',df_resumen.Confirmed.cast(IntegerType())) \\\n","    .withColumn('Deaths',df_resumen.Deaths.cast(IntegerType())) \\\n","    .withColumn('Recovered',df_resumen.Recovered.cast(IntegerType())) \\\n","    .withColumn('Active',df_resumen.Active.cast(IntegerType())) \\\n","    .withColumn('new_cases',df_resumen.new_cases.cast(IntegerType())) \\\n","    .withColumn('new_deaths',df_resumen.new_deaths.cast(IntegerType())) \\\n","    .withColumn('new_recovered',df_resumen.new_recovered.cast(IntegerType())) \\\n","    .withColumn('date',df_resumen.date.cast(DateType())) \\\n","    .withColumn('a単o',date_format(col('date'),'y')) \\\n","    .withColumn(\"mes\",date_format(col('date'),'MM')) \\\n","    .select('date','a単o','mes','region','country',col('province').alias('state'),'latitude','longitude' \\\n","            ,'Confirmed','Deaths','Recovered','Active','new_cases','new_deaths','new_recovered')\n","\n","    return dff_resumen\n","  #Funcion que guarda el archivo resumen en formato parquet particionado\n","  def writefile(self,df_resul,nomfile):\n","    df_resul.write.partitionBy('a単o','mes').parquet(self.ruta_curated+nomfile.replace('csv','parquet'))"],"metadata":{"id":"_zU0btA6tGwv","executionInfo":{"status":"ok","timestamp":1694850533541,"user_tz":300,"elapsed":1221,"user":{"displayName":"Everth Palomino","userId":"05447232039002194558"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["#Instancia la clase challenge y ejecuta la carga de los archivos.\n","if __name__ == \"__main__\":\n","  ruta_archivo = '/content/drive/MyDrive/Test_N5/archivos/'\n","  ruta_raw = '/content/drive/MyDrive/Test_N5/Raw/'\n","  ruta_curated = '/content/drive/MyDrive/Test_N5/Curated/'\n","  tipo_carga = 'Full' #Full\n","  fec_proceso = date.today()\n","  try:\n","    #Creacion del objeto de la clase Challenge\n","    file=Challenge(ruta_archivo,ruta_raw,ruta_curated,fec_proceso,tipo_carga)\n","    #Valida que existan archivos para cargar.\n","    val_file = file.valida_archivos_carga()\n","    if val_file:\n","      for x in range(len(val_file)):\n","        file.cargaraw(val_file[x])\n","    else:\n","      raise FileNotFoundError\n","\n","    #Lectura de archivos en formato parquet.\n","    df_covid19_clean=file.readfile('covid_19_clean_complete.parquet')\n","    df_country_wise_latest=file.readfile('country_wise_latest.parquet')\n","    df_grouped=file.readfile('full_grouped.parquet')\n","\n","    #Proceso de generacion de archivo resumen.\n","    df_resultado = file.proceso_data(df_grouped,df_covid19_clean)\n","\n","    #Guardado de resultado de archivo resumen particionado por a単o y mes.\n","    nomfile = f'resultado_covid_{fec_proceso}.parquet'\n","    file.writefile(df_resultado,nomfile)\n","\n","  except FileNotFoundError:\n","    print(f'No se encuentran archivos para cargar en la ruta {ruta_archivo}')"],"metadata":{"id":"92KWLHemu4k6","executionInfo":{"status":"ok","timestamp":1694850554413,"user_tz":300,"elapsed":14020,"user":{"displayName":"Everth Palomino","userId":"05447232039002194558"}}},"execution_count":41,"outputs":[]}]}